# ArXiv Digest Configuration

# Output Settings
output:
  base_dir: "./data" # Base directory for output files

# Paper Fetching Settings
arxiv:
  # Categories can be found at https://arxiv.org/category_taxonomy
  categories:
    # Compute Science Categories
    - "cs.AI" # Artificial Intelligence
    - "cs.CL" # Computation and Language
    - "cs.CV" # Computer Vision
    - "cs.LG" # Machine Learning
    - "cs.GL" # General Computer Science
    - "cs.HC" # Human-Computer Interaction
    - "cs.MA" # Multiagent Systems
    - "cs.NE" # Neural and Evolutionary Computing

    # Statistics Categories
    - "stat.AP" # Applications (Statistics)
    - "stat.CO" # Computation (Statistics)
    - "stat.ME" # Methodology (Statistics)
    - "stat.ML" # Machine Learning (Statistics)
    - "stat.OT" # Other (Statistics)
    - "stat.TH" # Theory (Statistics)

    # Physics Categories
    - "astro-ph.CO" # Cosmology and Nongalactic Astrophysics
    - "astro-ph.EP" # Earth and Planetary Astrophysics
    - "astro-ph.GA" # General Astrophysics
    - "astro-ph.HE" # High Energy Astrophysical Phenomena
    - "astro-ph.IM" # Instrumentation and Methods for Astrophysics
    - "astro-ph.SR" # Solar and Stellar Astrophysics
    - "cond-mat.dis-nn" # Statistical Mechanics (Condensed Matter)
    - "physics.data-an" # Data Analysis, Statistics and Probability (Physics)
    - "physics.ed-ph" # Education (Physics)
    - "physics.gen-ph" # General Physics
    - "physics.geo-ph" # Geophysics
    - "physics.space-ph" # Space Physics

  days_back: 1 # Days to look back for recent papers
  arxiv_days_only: true # If true, automatically shift search window if no papers found
  max_papers_per_category: 200 # Maximum papers to fetch per category
  filter_legacy_categories: true # Remove legacy categories from papers
  require_modern_categories: true # Exclude papers with only legacy categories
  request_delay: 1.0 # Rate limiting between API calls

# LLM-based Paper Scoring Settings
scoring:
  # Input file (null = auto-detect most recent arxiv_papers_*.json/csv)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Limit number of papers to score (null = score all papers)
  # Note: Papers are selected in order from arxiv fetch (first N papers)
  max_papers: null

  # Batch size for scoring (number of papers per LLM request, null = single paper per request)
  batch_size: 3

  # Retry configuration
  retry_attempts: 2

  # Metadata fields to include when scoring papers
  # Available: title, abstract, authors, categories, published, updated
  include_metadata:
    - title
    - abstract
    # - authors      # Uncomment to include author information
    # - categories   # Uncomment to include arXiv categories

  # Three-part prompt configuration for scoring
  # Part 1: Research context - describe the researcher's background and interests
  research_context_prompt: |
    You are evaluating research papers for Josh Speagle, an Assistant Professor of 
    Astrostatistics at the University of Toronto whose research motto is "Statistical AI 
    for Cosmic Discovery." His research focuses on four core areas:

    1. **Statistical Learning & AI**: Novel methods to discover patterns from large, messy datasets
    2. **Interpretability & Insight**: Rigorous understanding of methods to extract scientific insights
    3. **Inference & Computation**: Robust frameworks and algorithms to quantify what we can learn from data
    4. **Discovery & Understanding**: Applying methods to astronomical surveys to understand galaxy formation and evolution

    Josh values work that advances theoretical understanding of machine learning and statistics,
    introduces novel methodological approaches, demonstrates innovative scientific applications,
    announces significant observational/experimental results, and has potential to influence
    future research in ML/statistics, especially in astrophysics contexts.

  # Part 2: Scoring strategy - define what criteria to use for scoring
  scoring_strategy_prompt: |
    Evaluate papers using this three-component framework:

    **1. RELEVANCE (40% weight):**
    - Direct connection to astrophysics, AI/ML/statistics, or computational methods
    - Alignment with astrostatistics, interpretable AI, Bayesian methods, uncertainty quantification, or scientific applications
    - Cross-disciplinary potential bridging stats/astro/CS

    **2. CONTRIBUTION (30% weight):**
    - Novel techniques or significant improvements to existing methods
    - Theoretical rigor and mathematical soundness
    - Reproducibility, validation, and innovative AI/ML/statistics applications
    - Provides pedagogical insights that can be leveraged for students/training programs

    **3. IMPACT (30% weight):**
    - Directly applicable to current projects/surveys/challenges
    - Establishes foundations for follow-up work or expected to be heavily cited
    - Solves limiting problems or opens unexplored directions

  # Part 3: Score calculation - define numerical scale for scores
  score_calculation_prompt: |
    Rate each component from 1-10:
    - Relevance score (1-10): ___
    - Contribution score (1-10): ___
    - Impact score (1-10): ___

    Calculate: Final Score = (Relevance × 0.4) + (Contribution × 0.3) + (Impact × 0.3)
    Round to one decimal place.

    Be decisive about scores. Make sure to use the full scoring range effectively for each component.
    Scores should be calibrated so that a typical paper scores a 3 in each component
    (most papers are not directly relevant, make minor contributions, and have limited impact).
    Strong papers should score above a 7 in at least one component, with a final score above 5.
    Exceptional papers should score above 8 in at least two components, with a final score above 7.

# PDF Processing Settings
pdf_processing:
  # Input file (null = auto-detect most recent arxiv_papers_*_scored.json/csv)
  input_file: null

  # Paper Selection Configuration
  selection:
    # Percentile-based selection: top X% of papers by score
    percentile: 20

    # Score threshold: papers must also meet minimum score requirement
    score_threshold: 7.0

    # Quantity constraints: ensure reasonable number of papers
    min_papers: 5
    max_papers: 25

    # Random paper addition for exploration/serendipity
    # Add n_random papers from those not included in the original selection (0 = no random addition)
    # Sampling is based on softmax(scores / temperature) difference of 1 is a factor of e in odds
    # Temperature controls randomness: 0 = deterministic (top-N), 1 = score-based, null = uniform
    n_random: 5
    temperature: 1.0
    random_seed: null # null = random seed; only set if you want reproducibility for testing

  # PDF Download Configuration
  download:
    # Rate limiting (arXiv recommends delays between requests)
    rate_limit_delay: 3.0 # Seconds between downloads
    max_retries: 3 # Number of retry attempts for failed downloads
    timeout: 30 # Timeout in seconds for each download

    # Storage settings
    storage_dir: "./data/pdfs" # Directory to store downloaded PDFs
    organize_by_date: true # Organize PDFs in date-based subdirectories

    # File naming
    filename_format: "{arxiv_id}_{title_slug}.pdf" # Filename pattern (should use {arxiv_id}, {title_slug})
    max_filename_length: 100 # Maximum filename length (excluding extension)

    # Download verification
    verify_downloads: true # Verify PDF file integrity after download
    min_file_size: 10000 # Minimum file size in bytes

  # PDF Text Extraction Configuration
  extraction:
    # Extraction method selection
    method: "auto" # auto, pdfplumber, pymupdf, pypdf2, none (no extraction)
    fallback_methods: ["pymupdf", "pypdf2", "pdfplumber"] # Try these in order if primary fails

    # Content extraction options
    extract_text: true # Extract text content

    # Text processing options
    preserve_formatting: false # Keep original formatting vs. clean text
    remove_headers_footers: true # Remove page headers/footers
    min_text_length: 1000 # Minimum characters for valid extraction

    # Quality filtering
    max_extraction_errors: 5 # Skip papers with too many extraction errors
    require_abstract_keywords: true # Verify extraction quality using known abstracts

    # Output format for extracted text
    text_format: "plain" # plain, markdown, json
    include_metadata: true # Include extraction metadata in output

    # Storage options for extracted content
    save_extracted_text: true # Save extracted text to files
    text_storage_dir: "./data/extracted_text" # Directory for text files (mirrors PDF structure)
    cleanup_pdfs_after_extraction: true # Delete PDFs after successful extraction (ignored if none)

# LLM-based Paper Summarization Settings
summarization:
  # Input file (null = auto-detect most recent extraction_results_*.json or selected_papers_*.json)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Limit number of papers to summarize (null = summarize all papers)
  # Note: Papers are selected based on previous top scores
  max_papers: null

  # Retry configuration
  retry_attempts: 2

  # Text processing preferences
  prefer_extracted_text: true # Prefer extracted text over direct PDF processing

  # Token limits and chunking configuration (only applies to extracted text)
  max_single_chunk_tokens: 20000 # If extracted text fits in this, process as single chunk
  max_chunk_tokens: 15000 # Maximum tokens per chunk when chunking is needed
  chunk_overlap_ratio: 0.2 # Overlap between chunks (0.0-0.5)

  # Three-part prompt configuration for summarization
  # Part 1: Research context - describe the target audience and focus areas
  research_context_prompt: |
    You are summarizing research papers for Josh Speagle, an Assistant Professor of 
    Astrostatistics at the University of Toronto whose research motto is "Statistical AI 
    for Cosmic Discovery." His research focuses on four core areas:

    1. **Statistical Learning & AI**: Novel methods to discover patterns from large, messy datasets
    2. **Interpretability & Insight**: Rigorous understanding of methods to extract scientific insights
    3. **Inference & Computation**: Robust frameworks and algorithms to quantify what we can learn from data
    4. **Discovery & Understanding**: Applying methods to astronomical surveys to understand galaxy formation and evolution

    The summaries should be technical and comprehensive, suitable for an expert reader who wants to 
    understand the key methodological contributions, experimental validation, and broader implications.

  # Part 2: Summarization strategy - define the structure and focus areas
  summarization_strategy_prompt: |
    Create a comprehensive technical summary structured as follows:

    **1. CORE CONTRIBUTION (1 paragraph):**
    - Main innovation, finding, or theoretical advance
    - What makes this work novel or significant

    **2. METHODOLOGY (1-3 paragraphs):**
    - Key technical approaches and methods used
    - Novel algorithmic or experimental techniques
    - Important implementation details

    **3. RESULTS (1-3 paragraphs):**
    - Primary findings and their quantitative significance
    - Comparison with existing approaches or baselines
    - Statistical significance and validation methods

    **4. IMPLICATIONS (1 paragraph):**
    - Broader impact on the field or related areas
    - Future research directions or applications
    - Limitations or areas for improvement

    Not all sections may apply to every paper, but try to use this structure as a guide.
    You can adjust the number of paragraphs, the level of detail, and the focus areas based on the paper's
    topic, complexity, and length.

  # Part 3: Format and style guidelines
  summary_format_prompt: |
    Length: Aim for 1-2 pages (1000-2000 words maximum).
    Format: Use clear markdown structure with section headers.
    Style: Technical but accessible, suitable for expert readers.
    Focus: Emphasize reproducibility, methodology, and practical significance.

    Do not attempt to reproduce mathematical equations or complex formulas.
    Instead, describe them in words (e.g., "uses a modified loss function that combines...")
    Highlight connections to statistical methods, AI/ML techniques, and scientific applications.
