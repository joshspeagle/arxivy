# ArXiv Digest Configuration

# Output Settings
output:
  base_dir: "./data" # Base directory for output files

# Paper Fetching Settings
arxiv:
  # Categories can be found at https://arxiv.org/category_taxonomy
  categories:
    # Compute Science Categories
    - "cs.AI" # Artificial Intelligence
    - "cs.CL" # Computation and Language
    - "cs.CV" # Computer Vision
    - "cs.LG" # Machine Learning
    - "cs.GL" # General Computer Science
    - "cs.HC" # Human-Computer Interaction
    - "cs.MA" # Multiagent Systems
    - "cs.NE" # Neural and Evolutionary Computing

    # Statistics Categories
    - "stat.AP" # Applications (Statistics)
    - "stat.CO" # Computation (Statistics)
    - "stat.ME" # Methodology (Statistics)
    - "stat.ML" # Machine Learning (Statistics)
    - "stat.OT" # Other (Statistics)
    - "stat.TH" # Theory (Statistics)

    # Physics Categories
    - "astro-ph.CO" # Cosmology and Nongalactic Astrophysics
    - "astro-ph.EP" # Earth and Planetary Astrophysics
    - "astro-ph.GA" # General Astrophysics
    - "astro-ph.HE" # High Energy Astrophysical Phenomena
    - "astro-ph.IM" # Instrumentation and Methods for Astrophysics
    - "astro-ph.SR" # Solar and Stellar Astrophysics
    - "cond-mat.dis-nn" # Statistical Mechanics (Condensed Matter)
    - "physics.data-an" # Data Analysis, Statistics and Probability (Physics)
    - "physics.ed-ph" # Education (Physics)
    - "physics.gen-ph" # General Physics
    - "physics.geo-ph" # Geophysics
    - "physics.space-ph" # Space Physics

  days_back: 1 # Days to look back for recent papers
  arxiv_days_only: true # If true, automatically shift search window if no papers found
  max_papers_per_category: 200 # Maximum papers to fetch per category
  filter_legacy_categories: true # Remove legacy categories from papers
  require_modern_categories: true # Exclude papers with only legacy categories
  request_delay: 1.0 # Rate limiting between API calls

# LLM-based Paper Scoring Settings
scoring:
  # Input file (null = auto-detect most recent arxiv_papers_*.json/csv)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Limit number of papers to score (null = score all papers)
  # Note: Papers are selected in order from arxiv fetch (first N papers)
  max_papers: null

  # Batch size for scoring (number of papers per LLM request, null = single paper per request)
  batch_size: 3

  # Retry configuration
  retry_attempts: 2

  # Metadata fields to include when scoring papers
  # Available: title, abstract, authors, categories, published, updated
  include_metadata:
    - title
    - abstract
    # - authors
    # - categories

  # Three-part prompt configuration for scoring
  # Part 1: Research context - describe the researcher's background and interests
  research_context_prompt: |
    You are evaluating research papers for Josh Speagle, an Assistant Professor of 
    Astrostatistics at the University of Toronto whose research motto is "Statistical AI 
    for Cosmic Discovery." His research focuses on four core areas:

    1. **Statistical Learning & AI**: Novel methods to discover patterns from large, messy datasets
    2. **Interpretability & Insight**: Rigorous understanding of methods to extract scientific insights
    3. **Inference & Computation**: Robust frameworks and algorithms to quantify what we can learn from data
    4. **Discovery & Understanding**: Applying methods to astronomical surveys to understand galaxy formation and evolution

    Josh values work that advances theoretical understanding of machine learning and statistics,
    introduces novel methodological approaches, demonstrates innovative scientific applications,
    announces significant observational/experimental results, and has potential to influence
    future research in ML/statistics, especially in astrophysics contexts.

  # Part 2: Scoring strategy - define what criteria to use for scoring
  scoring_strategy_prompt: |
    Evaluate papers using this three-component framework:

    **1. RELEVANCE (40% weight):**
    - Direct connection to astrophysics, AI/ML/statistics, or computational methods
    - Alignment with astrostatistics, interpretable AI, Bayesian methods, uncertainty quantification, or scientific applications
    - Cross-disciplinary potential bridging stats/astro/CS

    **2. CONTRIBUTION (30% weight):**
    - Novel techniques or significant improvements to existing methods
    - Theoretical rigor and mathematical soundness
    - Reproducibility, validation, and innovative AI/ML/statistics applications
    - Provides pedagogical insights that can be leveraged for students/training programs

    **3. IMPACT (30% weight):**
    - Directly applicable to current projects/surveys/challenges
    - Establishes foundations for follow-up work or expected to be heavily cited
    - Solves limiting problems or opens unexplored directions

  # Part 3: Score calculation - define numerical scale for scores
  score_calculation_prompt: |
    Rate each component from 1-10:
    - Relevance score (1-10): ___
    - Contribution score (1-10): ___
    - Impact score (1-10): ___

    Calculate: Final Score = (Relevance × 0.4) + (Contribution × 0.3) + (Impact × 0.3)
    Round to one decimal place.

    Be decisive about scores. Make sure to use the full scoring range effectively for each component.
    Scores should be calibrated so that a typical paper scores a 3 in each component
    (most papers are not directly relevant, make minor contributions, and have limited impact).
    Strong papers should score above a 7 in at least one component, with a final score above 5.
    Exceptional papers should score above 8 in at least two components, with a final score above 7.

# PDF Processing Settings
pdf_processing:
  # Input file (null = auto-detect most recent arxiv_papers_*_scored.json/csv)
  input_file: null

  # Paper Selection Configuration
  selection:
    # Percentile-based selection: top X% of papers by score
    percentile: 20

    # Score threshold: papers must also meet minimum score requirement
    score_threshold: 7.0

    # Quantity constraints: ensure reasonable number of papers
    min_papers: 5
    max_papers: 25

    # Random paper addition for exploration/serendipity
    # Add n_random papers from those not included in the original selection (0 = no random addition)
    # Sampling is based on softmax(scores / temperature) difference of 1 is a factor of e in odds
    # Temperature controls randomness: 0 = deterministic (top-N), 1 = score-based, null = uniform
    n_random: 5
    temperature: 1.0
    random_seed: null # null = random seed; only set if you want reproducibility for testing

  # PDF Download Configuration
  download:
    # Rate limiting (arXiv recommends delays between requests)
    rate_limit_delay: 3.0 # Seconds between downloads
    max_retries: 3 # Number of retry attempts for failed downloads
    timeout: 30 # Timeout in seconds for each download

    # Storage settings
    storage_dir: "./data/pdfs" # Directory to store downloaded PDFs
    organize_by_date: true # Organize PDFs in date-based subdirectories

    # File naming
    filename_format: "{arxiv_id}_{title_slug}.pdf" # Filename pattern (should use {arxiv_id}, {title_slug})
    max_filename_length: 100 # Maximum filename length (excluding extension)

    # Download verification
    verify_downloads: true # Verify PDF file integrity after download
    min_file_size: 10000 # Minimum file size in bytes

  # PDF Text Extraction Configuration
  extraction:
    # Extraction method selection
    method: "auto" # auto, pdfplumber, pymupdf, pypdf2, none (no extraction)
    fallback_methods: ["pymupdf", "pypdf2", "pdfplumber"] # Try these in order if primary fails

    # Content extraction options
    extract_text: true # Extract text content

    # Text processing options
    preserve_formatting: false # Keep original formatting vs. clean text
    remove_headers_footers: true # Remove page headers/footers
    min_text_length: 1000 # Minimum characters for valid extraction

    # Quality filtering
    max_extraction_errors: 5 # Skip papers with too many extraction errors
    require_abstract_keywords: true # Verify extraction quality using known abstracts

    # Output format for extracted text
    text_format: "plain" # plain, markdown, json
    include_metadata: true # Include extraction metadata in output

    # Storage options for extracted content
    save_extracted_text: true # Save extracted text to files
    text_storage_dir: "./data/extracted_text" # Directory for text files (mirrors PDF structure)
    cleanup_pdfs_after_extraction: true # Delete PDFs after successful extraction (ignored if none)

# LLM-based Paper Summarization Settings
summarization:
  # Input file (null = auto-detect most recent extraction_results_*.json or selected_papers_*.json)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Limit number of papers to summarize (null = summarize all papers)
  # Note: Papers are selected based on previous top scores
  max_papers: null

  # Retry configuration
  retry_attempts: 2

  # Text processing preferences
  prefer_extracted_text: true # Prefer extracted text over direct PDF processing

  # Token limits and chunking configuration (only applies to extracted text)
  max_single_chunk_tokens: 18000 # If extracted text fits in this, process as single chunk
  max_chunk_tokens: 15000 # Maximum tokens per chunk when chunking is needed
  chunk_overlap_ratio: 0.2 # Overlap between chunks (0.0-0.5)

  # Three-part prompt configuration for summarization
  # Part 1: Research context - describe the target audience and focus areas
  research_context_prompt: |
    You are summarizing research papers for Josh Speagle, an Assistant Professor of 
    Astrostatistics at the University of Toronto whose research motto is "Statistical AI 
    for Cosmic Discovery." His research focuses on four core areas:

    1. **Statistical Learning & AI**: Novel methods to discover patterns from large, messy datasets
    2. **Interpretability & Insight**: Rigorous understanding of methods to extract scientific insights
    3. **Inference & Computation**: Robust frameworks and algorithms to quantify what we can learn from data
    4. **Discovery & Understanding**: Applying methods to astronomical surveys to understand galaxy formation and evolution

    The summaries should be technical and comprehensive, suitable for an expert reader who wants to 
    understand the key methodological contributions, experimental validation, and broader implications.

  # Part 2: Summarization strategy - define the structure and focus areas
  summarization_strategy_prompt: |
    Create a comprehensive technical summary structured as follows:

    **1. CORE CONTRIBUTION (1 paragraph):**
    - Main innovation, finding, or theoretical advance
    - What makes this work novel or significant

    **2. METHODOLOGY (1-3 paragraphs):**
    - Key technical approaches and methods used
    - Novel algorithmic or experimental techniques
    - Important implementation details

    **3. RESULTS (1-3 paragraphs):**
    - Primary findings and their quantitative significance
    - Comparison with existing approaches or baselines
    - Statistical significance and validation methods

    **4. IMPLICATIONS (1 paragraph):**
    - Broader impact on the field or related areas
    - Future research directions or applications
    - Limitations or areas for improvement

    Not all sections may apply to every paper, but try to use this structure as a guide.
    You can adjust the number of paragraphs, the level of detail, and the focus areas based on the paper's
    topic, complexity, and length.

  # Part 3: Format and style guidelines
  summary_format_prompt: |
    Length: Aim for 1-2 pages (800-1500 words maximum).
    Format: Use clear markdown structure with section headers.
    Style: Technical but concise, suitable for expert readers.
    Focus: Emphasize key findings, methodology, and practical significance.

    Do not attempt to reproduce mathematical equations or complex formulas.
    Instead, describe them concisely (e.g., "uses a modified loss function that combines...").
    Avoid verbose explanations and focus on the most important insights.

# LLM-based Paper Re-scoring Settings (Re-scoring with full summaries)
rescoring:
  # Input file (null = auto-detect most recent summarization_results_*.json)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Limit number of papers to rescore (null = rescore all papers)
  max_papers: null

  # Batch size for rescoring (number of papers per LLM request, null = single paper per request)
  batch_size: null

  # Retry configuration
  retry_attempts: 2

  # Metadata fields to include when rescoring papers
  include_metadata:
    - title
    - abstract
    - llm_summary # Include the full LLM-generated summary
    - summary_confidence # Confidence score of the summary
    # - llm_score # Original LLM score
    # - authors
    # - categories

  # Three-part prompt configuration for rescoring
  # Part 1: Research context (null = inherit from scoring.research_context_prompt)
  research_context_prompt: null

  # Part 2: Scoring strategy (null = inherit from scoring.scoring_strategy_prompt)
  scoring_strategy_prompt: null

  # Part 3: Score calculation (null = inherit from scoring.score_calculation_prompt)
  score_calculation_prompt: null

# LLM-based Paper Synthesis Settings
synthesis:
  # Input file (null = auto-detect most recent rescored_papers_*.json)
  input_file: null

  # Model configuration (must be defined in config/llm.yaml)
  model_alias: "deepseek-r1-qwen3"

  # Paper selection (similar to pdf_processing.selection)
  selection:
    score_field: "rescored_llm_score" # Which score to use for selection
    percentile: 35
    score_threshold: 7.5
    min_papers: 3
    max_papers: 8
    n_random: 2
    temperature: 0.5

  # Report generation
  generation:
    retry_attempts: 2

    # Metadata fields to include when synthesizing papers
    # Available: title, abstract, authors, categories, published, updated,
    #            llm_score, llm_explanation, rescored_llm_score, rescored_llm_explanation,
    #            llm_summary, summary_confidence, summary_source
    include_metadata:
      - title
      - abstract
      - authors
      - categories
      - llm_summary
      - summary_confidence
      - rescored_llm_score
      - rescored_llm_explanation

    # Two-part prompt configuration for synthesis
    # Part 1: Research context (null = inherit from scoring.research_context_prompt)
    research_context_prompt: null # Inherit from scoring config

    # Part 2: Synthesis strategy for final report
    report_prompt: |
      Create a comprehensive research synthesis (1500-2000 words) that organizes the selected papers into coherent themes and insights.

      Guidelines:
      - Use rescored scores to guide the depth of discussion and emphasis for each paper
      - Organize by conceptual themes and methodological approaches, not just by score ranking
      - Identify connections, trends, and complementary approaches across papers
      - Include practical implications and future research directions
      - Maintain technical accuracy while being accessible to an expert audience

      Structure suggestions (adapt based on the specific papers):
      - Executive summary of key themes
      - Thematic sections grouping related papers
      - Methodological insights and innovations
      - Research implications and future directions

      For each paper discussed, include: title, authors, key contributions, and relevance to the broader themes.
      Use the provided scores to emphasize the most significant contributions.

    # Example alternative format (commented out for reference)
    # podcast_prompt: |
    #   Create a 10-15 minute conversational podcast script for "AI Research Roundup."
    #
    #   Structure as an engaging audio experience:
    #   - Hook introduction highlighting the day's key themes
    #   - Use rescored scores to determine discussion depth and emphasis
    #   - Main segments organized by research themes (not score order)
    #   - Include natural transitions and audio cues: [PAUSE], [EMPHASIS], [TRANSITION]
    #   - Explain technical concepts clearly for audio consumption
    #   - Conclude with synthesis of implications and future directions
    #
    #   For each paper: mention title, authors, key finding, and why it matters.
    #   Maintain conversational tone while preserving technical accuracy.
